{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are now using cuda.\n",
      "496640\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from vqvae import VQVAE\n",
    "from torchaudio.datasets import VCTK_092\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn \n",
    "import os\n",
    "\n",
    "with_gpu = torch.cuda.is_available()\n",
    "\n",
    "if with_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print('We are now using %s.' % device)\n",
    "\n",
    "dataset = VCTK_092(root=r\"C:\\Users\\JadHa\\Desktop\\Uni\\VoiceConversion\", download=False)\n",
    "\n",
    "vqvae = VQVAE(in_channel=1).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in vqvae.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = optim.Adam(params=vqvae.parameters(), lr=3e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "speaker_id_list = dataset._speaker_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavenet_model import quantize_data\n",
    "dataset_train = torch.utils.data.Subset(dataset, np.arange(35121))\n",
    "dataset_val = torch.utils.data.Subset(dataset, np.arange(35121, len(dataset)))\n",
    "segment_length = int(2**15)\n",
    "\n",
    "def my_collate(batch):\n",
    "    wav_data = []\n",
    "    speaker_ids = []\n",
    "    labels = []\n",
    "    for wav, _, _, speaker_id, _ in batch :\n",
    "        wav_extended_len = int(segment_length * np.ceil(wav.shape[1]/segment_length))\n",
    "        wav_extended = torch.zeros((1,wav_extended_len))\n",
    "        wav_extended[:, :wav.shape[1]] = wav / wav.abs().max()\n",
    "        wav_extended = torch.stack(torch.split(wav_extended, split_size_or_sections=segment_length, dim=1), dim=0)\n",
    "        wav_data.append(wav_extended)\n",
    "        labels.append(torch.from_numpy(quantize_data(wav_extended, 256)).squeeze())\n",
    "        speaker_id_onehot = torch.zeros((wav_extended.shape[0], 128))\n",
    "        speaker_id_onehot[:, speaker_id_list.index(speaker_id)] = 1\n",
    "        speaker_ids.append(speaker_id_onehot)\n",
    "    return [torch.cat(wav_data, dim=0), torch.cat(labels, dim=0), torch.cat(speaker_ids, dim=0)]\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=2, shuffle=True, collate_fn=my_collate)\n",
    "val_loader = DataLoader(dataset_val, batch_size=2, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1, 32768])\n",
      "torch.Size([15, 32768])\n",
      "torch.Size([15, 128])\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n",
    "print(sample[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 323.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m vq_loss_weight \u001b[38;5;241m*\u001b[39m vq_loss\n\u001b[0;32m     18\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m , Training Loss : \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epoch_loss))\n",
      "File \u001b[1;32mc:\\Users\\JadHa\\anaconda3\\envs\\eeg_gnn\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JadHa\\anaconda3\\envs\\eeg_gnn\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 4.52 GiB is allocated by PyTorch, and 323.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "epochs = 10\n",
    "vq_loss_weight = 0.25\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    vqvae.train()\n",
    "    for batch_idx, (input, quantized_input, speaker_id) in enumerate(train_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        optimizer.zero_grad()\n",
    "        input = input.to(device)\n",
    "        quantized_input = quantized_input.to(device)\n",
    "        speaker_id = speaker_id.to(device)\n",
    "        output, vq_loss = vqvae(input, speaker_id)\n",
    "        recon_loss = criterion(output, quantized_input)\n",
    "        vq_loss = vq_loss.mean()\n",
    "        loss = recon_loss + vq_loss_weight * vq_loss\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch %d , Training Loss : %.2f \" %(epoch + 1, epoch_loss))\n",
    "    # Validation\n",
    "    vqvae.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, quantized_input, speaker_id) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            quantized_input = quantized_input.to(device)\n",
    "            speaker_id = speaker_id.to(device)\n",
    "            output, vq_loss = vqvae(input, speaker_id)\n",
    "            recon_loss = criterion(output, quantized_input)\n",
    "            vq_loss = vq_loss.mean()\n",
    "            loss = recon_loss + vq_loss_weight * vq_loss\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "    print(\"Epoch %d , Validation Loss : %.2f \" %(epoch + 1, epoch_loss))\n",
    "    chkpoint = {'model_state_dict': vqvae.state_dict()}\n",
    "    torch.save(chkpoint, os.path.join(\"saved_models\", \"vqvae_vctk.pt\"))\n",
    "    print('VQ-VAE is stored at folder:{}'.format('saved_models/'+'vqvae_vctk.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
